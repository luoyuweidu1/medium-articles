{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practice to Calculate and Interpret Feature Importance for RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**:         \n",
    "Having a model that accurately predicts outcome is great, but in many cases we also need to interpret our model. Let's say we just built a model to predict customers' renewal rate, in additional to knowing who will renew, it's perhaps equally important to understand which variables are importance in determining the renewal forecast to help improve our product and service.\n",
    "  \n",
    "People often use feature importances to interpret the model and most statistical models offer default calcuation in Sciki-learn package to make our lives easier. However, oftentime we can't trust the default feature importance. \n",
    "\n",
    "In this article, we are going to use the famous Titanic data and a random forest model to illustrate:\n",
    "- Why you need a **robust** model and **permutation importance** score to properly calculate feature importance.\n",
    "- Why you need to understand **features correlation** to properly interpret the feature importances.\n",
    "\n",
    "The practice described in this article can be generalized to other models as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:09.006904Z",
     "start_time": "2022-06-24T15:24:09.000167Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install rfpimp\n",
    "import linkedin.darwinfilemanager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, recall_score\n",
    "from distutils.version import LooseVersion\n",
    "import sklearn\n",
    "from sklearn.base import clone\n",
    "from rfpimp import *\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice to Calculate Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trouble with Default Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-22T15:14:07.467738Z",
     "start_time": "2022-06-22T15:14:07.462817Z"
    }
   },
   "source": [
    "We are going to use an example to show the **problem with the default impurity-based feature importance** provided in Sciki-learn for Random Forest model. The mechanism of default feature importance is calculated based on mean decrease in impurity (or gini importance), it measure how effective the features is at reducting uncertainty. See [this great article](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3) to illustrate the math behind the feature importance calculation.\n",
    "\n",
    "We download the famous Titanic dataset from [Kaggle](https://www.kaggle.com/c/titanic/data). The data set has passenger information for 1309 passgaes on Titanic and whether they ended up surviving the disaster. Here is brief description of columns included in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable      | Definition | Key|\n",
    "| ----------- | ----------- |-------|\n",
    "| survival      | Our response variable - Survival or not     |0 = No, 1 = Yes|\n",
    "| Pclass   | Ticket class |1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex|Sex||\n",
    "|Age|Age in years||\n",
    "|sibsp|# of siblings / spouses aboard the Titanic||\n",
    "|parch|# of parents / children aboard the Titanic||\n",
    "|ticket|Ticket number||\n",
    "|fare|Passenger fare||\n",
    "|cabin|Cabin number||\n",
    "|embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we separate the data into predictor set and response set. In the predictor set, we added two random variable random_cat and random_num. Since they are randomly generated, neither of them should show up as important features and they should roughly have similar ranks of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:16.167129Z",
     "start_time": "2022-06-24T15:24:16.065418Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('darwin://xianli-adhoc/titanic.csv')\n",
    "df = df.dropna(subset=['survived'])\n",
    "X = df.drop(\"survived\", axis = 1)\n",
    "y = df['survived']\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X[\"random_cat\"] = rng.randint(3, size=X.shape[0])\n",
    "X[\"random_num\"] = rng.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we did some simple cleaning and transformation on the data. This is not the focus of this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:25.065072Z",
     "start_time": "2022-06-24T15:24:25.044822Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\n",
    "numerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n",
    "\n",
    "#did a simple imputation to fill missing categorical var with mode. The only var missing is embarked with 2 missing values \n",
    "X['embarked'] = X['embarked'].fillna(X['embarked'].mode()[0])\n",
    "enc = OrdinalEncoder()\n",
    "X[categorical_columns] = enc.fit_transform(X[categorical_columns] )\n",
    "\n",
    "#imputed numerical var missing values with mean \n",
    "X[numerical_columns] = X[numerical_columns].apply(lambda x: x.fillna(np.mean(x)))\n",
    "\n",
    "X = X[categorical_columns + numerical_columns]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we build a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:28.043678Z",
     "start_time": "2022-06-24T15:24:27.756505Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 1,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:30.146072Z",
     "start_time": "2022-06-24T15:24:29.935316Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\n",
    "print(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model is overfitting on training data but still have decent performance on testing set. Let's use this model for now to illustrate some pitfalls of default feature importance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:32.105694Z",
     "start_time": "2022-06-24T15:24:31.849220Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index = X_train.columns).sort_values(ascending = True)\n",
    "feat_importances.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the default feature importance, we notice that:\n",
    "- The random_num has a larger importance score in comparison with random_cat, which confirms that **impurity-based importances are biased towards high cardinality and numerical features**.\n",
    "- Non-predictive random_num variable is ranked as one of the most importance features, which is very suspicious. This reflects the **default features importance is not accurate when you have an overfitting model**. When a model overfits, it's picking up too much noise from training set that its ability to make generalized prediction on the test has been reduced. When this happens,the feature importance is not reliable as it's calculated based on training set. More generally, **it only makes sense to look at feature importance when you have a model that can decently predict.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importances to Rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then how do we appropriately calculate the importance of features. One way is to use permutation importances scores. It's calculated with following steps: \n",
    "1. Train a baseline model and record the score (we use accuracy in this example) on the validation set. \n",
    "2. Re-shuffle values for one feature, use the model to predict again and calculate score on validation set. The feature importance is the difference between the baseline in 1 and the permutation score in 2.\n",
    "3. Repeat the process for all features.\n",
    "\n",
    "Here we leveraged `permutation_importance` function added to `Scikit-learn` package in 2019. When calling the function, we set the n_repeats = 20 which meant for each variable, we randomly shuffle 20 times and calculate the decrease in accuracy 20 times to create the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:55.721731Z",
     "start_time": "2022-06-24T15:24:33.404818Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "#calculate permutation importance for test data \n",
    "result_test = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_test = result_test.importances_mean.argsort()\n",
    "importances_test = pd.DataFrame(\n",
    "    result_test.importances[sorted_importances_idx_test].T,\n",
    "    columns=X.columns[sorted_importances_idx_test],\n",
    ")\n",
    "\n",
    "#calculate permutation importance for training data \n",
    "result_train = permutation_importance(\n",
    "    rf, X_train, y_train, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_train = result_train.importances_mean.argsort()\n",
    "importances_train = pd.DataFrame(\n",
    "    result_train.importances[sorted_importances_idx_train].T,\n",
    "    columns=X.columns[sorted_importances_idx_train],\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "importances_test.plot.box(vert=False, whis=10, ax = axs[0])\n",
    "axs[0].set_title(\"Permutation Importances (test set)\")\n",
    "axs[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[0].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[0].figure.tight_layout()\n",
    "\n",
    "importances_train.plot.box(vert=False, whis=10, ax = axs[1])\n",
    "axs[1].set_title(\"Permutation Importances (train set)\")\n",
    "axs[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[1].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[1].figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `sex` and `pclass` shows as the most important features and `random_cat` and `random_num` are no longer have high importance scores based on permutation importance on test set. The box plot shows the distribution of decrease in accuracy score in N repeat permutation (N = 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute permutation imporatnce on training set. This shows that `random_num` and `random_cat` get a significantly higher importance ranking than when computed on the test set and the ranking of features have shifted significantly. As noted before, it's due to the overfit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why Sciki-learn still includes a default calculation for feature importance given it's not accurate. Breiman and Cutler, the inventors of RFs, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.” So the default is meant to serve as a proxy for permutation importance. However, as Strobl et al pointed out in `Bias in random forest variable importance measures`: Illustrations, sources and a solution that “the variable importance measures of Breiman's original Random Forest method ... are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Robost Model is the Pre-requisite for Accurate Importance Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply some level of regularization by setting min_samples_leaf = 20 instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:56.003200Z",
     "start_time": "2022-06-24T15:24:55.723751Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 20,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:24:56.214802Z",
     "start_time": "2022-06-24T15:24:56.004719Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\n",
    "print(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we adjust the model parameters to avoid overfitting, we notice that accuracy now are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:17.815477Z",
     "start_time": "2022-06-24T15:24:56.216677Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate permutation importance for test data \n",
    "result_test = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_test = result_test.importances_mean.argsort()\n",
    "importances_test = pd.DataFrame(\n",
    "    result_test.importances[sorted_importances_idx_test].T,\n",
    "    columns=X.columns[sorted_importances_idx_test],\n",
    ")\n",
    "\n",
    "#calculate permutation importance for training data \n",
    "result_train = permutation_importance(\n",
    "    rf, X_train, y_train, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_train = result_train.importances_mean.argsort()\n",
    "importances_train = pd.DataFrame(\n",
    "    result_train.importances[sorted_importances_idx_train].T,\n",
    "    columns=X.columns[sorted_importances_idx_train],\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "importances_test.plot.box(vert=False, whis=10, ax = axs[0])\n",
    "axs[0].set_title(\"Permutation Importances (test set)\")\n",
    "axs[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[0].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[0].figure.tight_layout()\n",
    "\n",
    "importances_train.plot.box(vert=False, whis=10, ax = axs[1])\n",
    "axs[1].set_title(\"Permutation Importances (train set)\")\n",
    "axs[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[1].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[1].figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the overfit, the features importance calculated on training and testing set look much similar to each other. This gives us more confidence that a robust model gives more accurate model importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to calculate accurate feature importance is **drop-column importance**. It's the most accurate way to calculate feature importance. The idea is to calculate the model performance with all predictors and drop the signal predictor and see the reduction in performance. The more important the feature is, the higher the reduction we see in the model performance. \n",
    "\n",
    "Given the high computation cost of drop-column importance (we need to retain the a model for each variable), we generally prefer permutation Importance score. But it's an excellent way to validate our permutation importance. The importance values could be difference between the two strategies, but the order of feature importances should be roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:17.825480Z",
     "start_time": "2022-06-24T15:25:17.817216Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropcol_importances(rf, X_train, y_train):\n",
    "    rf_ = clone(rf)\n",
    "    rf_.random_state = 42\n",
    "    rf_.fit(X_train, y_train)\n",
    "    \n",
    "    #use out of bag error as performance measurement\n",
    "    baseline = rf_.oob_score_\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        X = X_train.drop(col, axis=1)\n",
    "        rf_ = clone(rf)\n",
    "        rf_.random_state = 42\n",
    "        rf_.fit(X, y_train)\n",
    "        o = rf_.oob_score_\n",
    "        imp.append(baseline - o)\n",
    "    imp = np.array(imp)\n",
    "    I = pd.DataFrame(\n",
    "            data={'Feature':X_train.columns,\n",
    "                  'Importance':imp})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=True)\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:20.623447Z",
     "start_time": "2022-06-24T15:25:17.827111Z"
    }
   },
   "outputs": [],
   "source": [
    "imp = dropcol_importances(rf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:35.309319Z",
     "start_time": "2022-06-24T15:25:35.174041Z"
    }
   },
   "outputs": [],
   "source": [
    "imp.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rankings of the features are similar to permutation features although the magnitude is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-22T16:05:32.183884Z",
     "start_time": "2022-06-22T16:05:32.179866Z"
    }
   },
   "source": [
    "### Best Practice to interpret feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge of Feature Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have a robust model and correctly implement the right strategy to calculate feature importance, we can move forward to the interpretation. Can we argue that one feature is 10X more importance than another? It depends.   \n",
    "\n",
    "At this stage, correlation is the biggest challenge for us to interpret the feature importance. The assumption we make so far consider each feature individually. If all features are totally independent and not correlated in any way, it would easy to make the interpretation. However, if two or more features are collinear, it will affect the feature importance result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-22T16:10:12.108174Z",
     "start_time": "2022-06-22T16:10:12.103343Z"
    }
   },
   "source": [
    "To illustrate this, let's use an extreme example and duplicate the column sex to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:39.427407Z",
     "start_time": "2022-06-24T15:25:39.147623Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train['sex_dedupe'] = X_train['sex'].copy()\n",
    "X_test['sex_dedupe'] = X_test['sex'].copy()\n",
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 20,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:41.458374Z",
     "start_time": "2022-06-24T15:25:41.247261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\n",
    "print(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-22T16:10:28.565168Z",
     "start_time": "2022-06-22T16:10:28.561376Z"
    }
   },
   "source": [
    "The model performance slightly decreases as we added an feature that didn't add much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:25:54.127514Z",
     "start_time": "2022-06-24T15:25:43.181786Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_train.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()\n",
    "#show the original graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-22T16:11:26.061098Z",
     "start_time": "2022-06-22T16:11:26.056591Z"
    }
   },
   "source": [
    "We see now the importance attributed to sex features are distributed between two duplicates sex columns. What happen if we add some noises to the duplicate column? Let's try adding a random noise ranging from 0-1 to the sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:26:11.027254Z",
     "start_time": "2022-06-24T15:25:59.908661Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding noise to the features \n",
    "X_train = X_train.drop(\"sex_dedupe\", axis = 1)\n",
    "X_test = X_test.drop(\"sex_dedupe\", axis = 1)\n",
    "\n",
    "noise_train = np.random.random(len(X_train['sex']))*1\n",
    "noise_test = np.random.random(len(X_test['sex']))*1\n",
    "\n",
    "X_train['sex_noisy'] = X_train['sex'] + noise_train\n",
    "X_test['sex_noisy'] = X_test['sex'] + noise_test\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 20,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "result = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_train.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sex_noisy` now is the most importance variable. What happen if we increase the magnitude of noise added? Let's increase the random varaible to ranging from 0-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:26:26.531269Z",
     "start_time": "2022-06-24T15:26:15.461061Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adding noise to the features \n",
    "#Increse the magnitude of noise\n",
    "noise_train = np.random.random(len(X_train['sex']))*3\n",
    "noise_test = np.random.random(len(X_test['sex']))*3\n",
    "\n",
    "X_train['sex_noisy'] = X_train['sex'] + noise_train\n",
    "X_test['sex_noisy'] = X_test['sex'] + noise_test\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 20,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "result = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_train.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see with more noise added, `sex_noisy` now no longer rank as the top predictors and `sex` is back to the top. The conclusion is that permutation importance computed on random forest model **spreads importance across collinear variables**. The amount of sharing appears to be a function of how much noise there in between the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Collinear Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at correlation between features. We use `feature_corr_matrix` from rfpimp package, which gives us the Spearman's correlation. The difference between Spearman's correlation and standard Pearson's correlation is that Spearman's correaltion first converts two variables to rank values and then run Peason correlation on ranked values. It doesn't assume a linear relationship between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:26:30.930538Z",
     "start_time": "2022-06-24T15:26:30.907403Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_corr_matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:26:33.855489Z",
     "start_time": "2022-06-24T15:26:33.187093Z"
    }
   },
   "outputs": [],
   "source": [
    "from rfpimp import plot_corr_heatmap\n",
    "viz = plot_corr_heatmap(X_train, figsize=(7,5))\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pclass` is highly correlated with `fare`, which is not too surprising as class of cabin depends on how much money you pay for it. It happens quite often in business that we use multiple features that are correlated with each other in the model for prediction. From the previous example, we see that when two or multiple are collinear, the importance computed are shared across collinear var based on the info to noise ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1: Combine Collinear Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to tackle this is to combine features that are highly collinear with each other to form a feature family and we can say this feature family together ranked as the X most importance features. To to that, we will use `rfpimp` package that allows us to permutate two variables at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:28:47.486854Z",
     "start_time": "2022-06-24T15:28:46.362757Z"
    }
   },
   "outputs": [],
   "source": [
    "import rfpimp as rfpimp\n",
    "features = [['pclass','fare'],'sex','embarked','age','sibsp','parch']\n",
    "X_train = X_train.drop(['sex_noisy','random_num','random_cat'], axis = 1)\n",
    "X_test = X_test.drop(['sex_noisy','random_num','random_cat'], axis = 1)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "         n_estimators=100,\n",
    "         n_jobs=-1,\n",
    "         min_samples_leaf = 20,\n",
    "         oob_score=True,\n",
    "         random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "I = rfpimp.importances(rf, X_test, y_test, features=features)\n",
    "plot_importances(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2: Remove Highly Collinear Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a feature is dependent on other features that means the features can be accurately predicted using all other features as independe variables. Higher the model R^2 is , the more dependent feature is, and the more confidence we can remove the variable without sacrificing on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:28:52.752167Z",
     "start_time": "2022-06-24T15:28:50.966793Z"
    }
   },
   "outputs": [],
   "source": [
    "dependence_matrix = feature_dependence_matrix(X_train,\n",
    "                          rfrmodel=RandomForestRegressor(n_estimators=50, oob_score=True),\n",
    "                          rfcmodel=RandomForestClassifier(n_estimators=50, oob_score=True),\n",
    "                          cat_count=20,\n",
    "                          zero=0.001,\n",
    "                          sort_by_dependence=False,\n",
    "                          n_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:28:54.917920Z",
     "start_time": "2022-06-24T15:28:54.907915Z"
    }
   },
   "outputs": [],
   "source": [
    "dependence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution_time": {
     "end_time": "2022-06-24T15:28:57.792799Z",
     "start_time": "2022-06-24T15:28:57.420028Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_dependence_heatmap(dependence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column dependence shows the dependence score and the feature is completely predictable using other other features the value is close to 1, which means it could be dropped without affecting accuracy. In this case, we can probably drop one of the pclass and fare without affecting much accruacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we 1)have a robust model and implement the right strategy to calculate permutation importance and 2)deal with feature correlation, we can start crafting our message to share with stakeholder. \n",
    "\n",
    "Going back the common question people ask \"Is feature 1 10x more importance feature 2?\", you may understand at this moment that we have confidence to make the argument only when all the features are independent or have very low correlation. But in the real world, that's rarely the case. The recommended strategy is to group features to High, Medium and Low impact tier, without focusing too much on the exact magnitude. If we really need to show the relatively comparison between features, try to group collinear features (or drop them) and interpret based on category to make the argument more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://explained.ai/rf-importance/    \n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html   \n",
    "https://github.com/parrt/random-forest-importances   \n",
    "https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n",
    "https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "darwin": {
   "resource_id": 960065,
   "username": "xianli"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}